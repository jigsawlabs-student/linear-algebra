{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpacking SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll learn some of the linear algebra fundamentals that explains how principal component analysis works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular value decomposision says that any matrix $A$ can be decomposed into three separate matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*tmVzY_1k9_JpxyKDkXwAQA.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any matrix can be thought of as rotating and stretching space.  We can see this through the illustration, [courtesy of Wikipedia](https://en.wikipedia.org/wiki/Singular_value_decomposition), below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./matrix-transformation.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what singular value decomposition performs, is to translate these rotation and stretching into three different matrices $USV$, also known as $U \\Sigma V$.  The matrix $\\Sigma$ is diagonal matrix, responsible for stretching different dimensions of space.  And the matrices $U$ and $V$ are two orthonormal matrices which rotate space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./svd.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [image below](https://en.wikipedia.org/wiki/Singular_value_decomposition), we can see that original matrix $M$ as performs both a stretch and a rotation of the circle.  Meanwhile, $V\\Sigma U$ decomposes this transformation into three steps.  $V$ first rotates the circle, then $\\Sigma$ stretches different dimensions of the circle, and finally $U$ performs another rotation.  Together, the transformations performed by $U \\Sigma V$ are precisely the same transformations performed by $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing it in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we'll get into how this $SVD$ can help us, and what it has to do with PCA.  But first let's see it in action.\n",
    "\n",
    "Here, we'll initialize a matrix $A$, and then we'll use SVD to break it down into three different matrices.  Ok, let's initialize our matrix $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.array([[3, 2, 2], \n",
    "              [2, 3, -2],\n",
    "              [1, 4, 6]\n",
    "             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, we can think of applying a matrix as both a stretch and rotation of space.  And we can use svd to break this down into three matrices that separately perform a rotatation, followed by a stretch, and then another rotation. We do so with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.44598142, -0.3315123 , -0.83138449],\n",
       "        [-0.11179123, -0.90097181,  0.41922848],\n",
       "        [-0.88803339,  0.27990961,  0.36475652]]),\n",
       " array([8.05000987, 4.39570781, 1.6956102 ]),\n",
       " array([[-0.30429281, -0.59372226, -0.74491594],\n",
       "        [-0.57250642, -0.51102159,  0.64116561],\n",
       "        [-0.76134243,  0.62157124, -0.18440961]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "U, S, V = svd(A)\n",
    "U, S, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're left with these three matrices.  Which are factorizations of our original matrix.  Let's put our pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  2.,  2.],\n",
       "       [ 2.,  3., -2.],\n",
       "       [ 1.,  4.,  6.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import diag\n",
    "diag_S = diag(S)\n",
    "U.dot(diag_S).dot(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why it matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why this is useful is because our whole goal with principal component analysis is to break down our matrix, and somehow retain the components that are useful.  Let's take a look at the three matrices provided by SVD again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.44598142, -0.3315123 , -0.83138449],\n",
       "        [-0.11179123, -0.90097181,  0.41922848],\n",
       "        [-0.88803339,  0.27990961,  0.36475652]]),\n",
       " array([[8.05000987, 0.        , 0.        ],\n",
       "        [0.        , 4.39570781, 0.        ],\n",
       "        [0.        , 0.        , 1.6956102 ]]),\n",
       " array([[-0.30429281, -0.59372226, -0.74491594],\n",
       "        [-0.57250642, -0.51102159,  0.64116561],\n",
       "        [-0.76134243,  0.62157124, -0.18440961]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, diag_S, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the diagonal matrix $\\Sigma$, we can see that the diagonal values are ordered from largest to smallest.  In SVD, these values are called our **singular values**, and we order them this in descending order by convention.\n",
    "\n",
    "Now let's think about what it means to multiply these three matrices together.  We can rewrite $USV^T$ as the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A = U \\Sigma V =  \\sigma_1 u_1 \\cdot v_1^T + \\sigma_2  u_2 \\cdot v_2^T +  \\sigma_3  u_3 \\cdot v_3^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a series of dot products, each multiplied by a singular values $\\sigma$.  And SVD says that adding these three dot products together, should bring us back to our original matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.00761926,  1.89608714,  2.08766964],\n",
       "       [ 2.0953212 ,  2.99450952, -1.90722022],\n",
       "       [ 0.9841705 ,  3.8605498 ,  6.09318889]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8.05*U[:, :1].dot(V[:, :1].T) + 4.395*U[:, 1:2].dot(V[:, 1:2].T) + 1.6956102*U[:, 2:3].dot(V[:, 2:3].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  2,  2],\n",
       "       [ 2,  3, -2],\n",
       "       [ 1,  4,  6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the reason why this is useful, is because above, it is almost like we are using these three dot products to rebuild our matrix in layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The first dot product gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.09245696, 2.05538416, 2.73333384],\n",
       "       [0.27383901, 0.51520964, 0.68514683],\n",
       "       [2.17528849, 4.09265873, 5.44258481]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8.05*U[:, :1].dot(V[:, :1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And adding in the second dot product brings us even closer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.95750826,  2.79994087,  1.82770668],\n",
       "       [ 2.62484327,  2.53873818, -1.776133  ],\n",
       "       [ 1.44488974,  3.46399857,  6.20724345]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8.05*U[:, :1].dot(V[:, :1].T) + 4.395*U[:, 1:2].dot(V[:, 1:2].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means that if we lost one of the vectors, then we still *come close* to reproducing our matrix A.  So we can see that we do not perfectly reproduce A, here, but we are able to reduce our dimensions and capture most of the movement in A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So now, let's reduce our data into a series $2 x 2$ matrices, and we can see that we can still come close to reproducing our original matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 2), (2, 2), (3, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_star, Sigma_star, V_star = U[:, :2], diag_S[:2, :2], V[:, :2]\n",
    "\n",
    "U_star.shape, Sigma_star.shape, V_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.95764892,  2.8000633 ,  1.82756418],\n",
       "       [ 2.62522223,  2.5390647 , -1.77652855],\n",
       "       [ 1.44477478,  3.46390234,  6.20737327]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_star.dot(Sigma_star).dot(V_star.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is where SVD becomes valuable.  It means that we can use SVD to come close to reproducing the original matrix with fewer features than we had originally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much do we lose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice thing about singular value decomposition is that we can see how much information we lose as we reduce the number of features that we have.\n",
    "\n",
    "The amount that we lose is purely determined by the size of the singular values.  So, in our example above, our singular values are:\n",
    "\n",
    "`8.05000987, 4.39570781, 1.6956102`.  This means that if we just retain two of the vectors or \"components\" we would retain 88 percent of the information of our original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8800954044493875"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(8.05000987 + 4.39570781)/(8.05000987 + 4.39570781 + 1.6956102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why this is the case is because when we multiply $U \\Sigma V$, $U$ and $V$ are orthogonal matrices, meaning that each vector has the same length of 1, and so the amount of contribution of each dot product is purely determined by the size of the singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.09245696, 2.05538416, 2.73333384],\n",
       "       [0.27383901, 0.51520964, 0.68514683],\n",
       "       [2.17528849, 4.09265873, 5.44258481]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_component = 8.05*U[:, :1].dot(V[:, :1].T)\n",
    "first_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8650513 ,  0.74455671, -0.90562717],\n",
       "       [ 2.35100426,  2.02352854, -2.46127984],\n",
       "       [-0.73039875, -0.62866016,  0.76465863]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_component = 4.395*U[:, 1:2].dot(V[:, 1:2].T)\n",
    "second_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.049999999999999, 4.394999999999996)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "norm(first_component), norm(second_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is useful because it allows us to see tradeoff of only retaining some of the features in the new matrices returned from SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned some of the main components of singular value decomposition.  We saw that SVD allows us to take a matrix A, and break it into three different matrices $A = U\\Sigma V$.  And then we saw that the product of these three matrices can be thought of as the sum of three dot products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A = U \\Sigma V =  \\sigma_1 u_1 \\cdot v_1^T + \\sigma_2  u_2 \\cdot v_2^T +  \\sigma_3  u_3 \\cdot v_3^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding these three dot products together recreates the original matrix completely.  But more useful, is that by removing one or more of these dot products, we can reduce the number of features in our matrices, and still retain a portion of the data in the original matrix A.  So for example, we saw we limited our matrices $U \\Sigma V$ from three columns to two:\n",
    "\n",
    "$A^* = U^*\\Sigma^*V^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, because U and V are both orthonormal matrices the size, and thus contributions of each cross product is purely determined by the size of the singular values, $\\sigma$.  This means that we can assess the amount of information lost by limiting the number of vectors in $U$ $\\Sigma$ and $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PCA vs SVD](https://intoli.com/blog/pca-and-svd/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
